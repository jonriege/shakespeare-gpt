{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.python.ops import bert_tokenizer\n",
    "from tensorflow_text.tools.wordpiece_vocab import wordpiece_tokenizer_learner_lib as learner\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the source text from the concatenated [concatenated works of Shakespeare](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae3725-98c0-4d8c-a65f-11b45f5f4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.RAW_DATA_PATH, 'r') as file:\n",
    "    shakespeare_plays = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d48751-ee20-4603-b3aa-8b6c9bb1aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "sample = shakespeare_plays[:147]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26cad1-4cde-481e-9f37-cc3c9c1ba07f",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bb6677-5149-4191-81e7-75251b1cbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert_tokenizer.BasicTokenizer(**config.BERT_TOKENIZER_PARAMS)\n",
    "words_dataset = tokenizer.tokenize(shakespeare_plays)\n",
    "word_counts = learner.count_words(words_dataset)\n",
    "vocab = learner.learn(word_counts, config.VOCAB_SIZE, config.RESERVED_TOKENS, **config.LEARN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00f8691-e70b-4436-94c6-16abed4ca6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '$', '&', \"'\", ',', '-']\n",
      "['well', 'was', 'which', 'there', 'how', 'am', 'then', '##ed', '##ing', 'man']\n",
      "['##.', '##3', '##:', '##;', '##?', '##[', '##]', '##j', '##q', '##v']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(vocab[100:110])\n",
    "print(vocab[-10:])\n",
    "\n",
    "with open(config.VOCAB_PATH, \"w\") as f:\n",
    "    for token in vocab:\n",
    "        print(token, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571dd1f-41bd-4dfc-a890-a7a3b94a25ae",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bab5441-d04e-4751-bc92-c7cfc091a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.BertTokenizer(config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947351b8-d3ca-4c59-827e-715508bb0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[140, 606, 12, 196, 76, 1417, 178, 539, 8, 170, 53, 147, 10, 72, 12, 147,\n",
      "  8, 147, 10, 140, 606, 12, 47, 80, 72, 1917, 361, 45, 269, 115, 45, 4344,\n",
      "  14]]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample).merge_dims(-2, -1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18d6633-f624-4e86-ad8d-37e1400da6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen : you are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tf.gather(vocab, tokens)\n",
    "txt_tokens = tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1).numpy()[0].decode(\"utf-8\")\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the GPT, including functionality for cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=\" \", axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(config.VOCAB_PATH)\n",
    "\n",
    "        vocab = pathlib.Path(config.VOCAB_PATH).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(config)\n",
    "tf.saved_model.save(tokenizer, config.TOKENIZER_PATH)\n",
    "reloaded_tokenizer = tf.saved_model.load(config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 647,  650,  736,   63,  866, 2003, 4975,    4]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizer.tokenize([\"Hello TensorFlow!\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizer.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5690d-59e3-45e5-ae17-5e3f6ff51f5b",
   "metadata": {},
   "source": [
    "## Save tokenized dataset\n",
    "TODO: Split tokenized dataset into batches and then separate into train and validation sets. Store the resulting tokenized datasets on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee06791-99d4-4d23-b9af-6af7bead0565",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mshuffle(config\u001b[38;5;241m.\u001b[39mBUFFER_SIZE)\u001b[38;5;241m.\u001b[39mtake(config\u001b[38;5;241m.\u001b[39mN_SENTENCES)\n\u001b[1;32m      2\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(config\u001b[38;5;241m.\u001b[39mN_SENTENCES, n_samples)\n\u001b[1;32m      3\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_samples \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mVALIDATION_SHARE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(config.BUFFER_SIZE).take(config.N_SENTENCES)\n",
    "n_samples = min(config.N_SENTENCES, n_samples)\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659455d1-05ac-458b-a338-15e209d89f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, nor in train_dataset.take(1):\n",
    "    print(f\"English: {eng.numpy().decode('utf-8')}\")\n",
    "    print(f\"Norwegian: {nor.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03d25-340e-45b6-bd12-920d98f3e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
