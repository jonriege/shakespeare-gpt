{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.python.ops import bert_tokenizer\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    wordpiece_tokenizer_learner_lib as learner,\n",
    ")\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the source text from the concatenated [concatenated works of Shakespeare](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt). Replace newline symbols (\\n) with a special token so that information about the structure of the text is correctly tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae3725-98c0-4d8c-a65f-11b45f5f4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.RAW_DATA_PATH, \"r\") as file:\n",
    "    shakespeare_plays = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d48751-ee20-4603-b3aa-8b6c9bb1aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "sample = shakespeare_plays[:147]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9187a68e-5d92-402f-88be-6e18bd1961db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: NEWLINE Before we proceed any further, hear me speak. DOUBLEN All: NEWLINE Speak, speak. DOUBLEN First Citizen: NEWLINE You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "shakespeare_plays = shakespeare_plays.replace(\"\\n\\n\", config.DOUBLEN_TOKEN)\n",
    "shakespeare_plays = shakespeare_plays.replace(\"\\n\", config.NEWLINE_TOKEN)\n",
    "sample = shakespeare_plays[:185]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26cad1-4cde-481e-9f37-cc3c9c1ba07f",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bb6677-5149-4191-81e7-75251b1cbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert_tokenizer.BasicTokenizer(**config.BERT_TOKENIZER_PARAMS)\n",
    "words_dataset = tokenizer.tokenize(shakespeare_plays)\n",
    "word_counts = learner.count_words(words_dataset)\n",
    "vocab = learner.learn(\n",
    "    word_counts,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.RESERVED_TOKENS,\n",
    "    **config.LEARN_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00f8691-e70b-4436-94c6-16abed4ca6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';']\n",
      "['shall', 'are', 'To', 'thee', 'by', 'we', 'That', 'on', 'no', 'our']\n",
      "['##U', '##V', '##W', '##X', '##Z', '##[', '##]', '##j', '##q', '##v']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(vocab[100:110])\n",
    "print(vocab[-10:])\n",
    "\n",
    "with open(config.VOCAB_PATH, \"w\") as f:\n",
    "    for token in vocab:\n",
    "        print(token, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571dd1f-41bd-4dfc-a890-a7a3b94a25ae",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bab5441-d04e-4751-bc92-c7cfc091a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.BertTokenizer(\n",
    "    config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947351b8-d3ca-4c59-827e-715508bb0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[205, 698, 8, 65, 992, 105, 2366, 221, 641, 4, 222, 77, 184, 6, 66, 355,\n",
      "  8, 65, 804, 4, 184, 6, 66, 205, 698, 8, 65, 151, 101, 99, 2311, 467, 69,\n",
      "  325, 153, 69, 5270, 10]]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample).merge_dims(-2, -1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18d6633-f624-4e86-ad8d-37e1400da6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen : NEWLINE Before we proceed any further , hear me speak . DOUBLEN All : NEWLINE Speak , speak . DOUBLEN First Citizen : NEWLINE You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tf.gather(vocab, tokens)\n",
    "txt_tokens = (\n",
    "    tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1)\n",
    "    .numpy()[0]\n",
    "    .decode(\"utf-8\")\n",
    ")\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a17cd2d-3f7b-4bb8-8fc1-b685e76e9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'First', b'Citizen', b':', b'NEWLINE', b'Before', b'we', b'proceed',\n",
      "  b'any', b'further', b',', b'hear', b'me', b'speak', b'.', b'DOUBLEN',\n",
      "  b'All', b':', b'NEWLINE', b'Speak', b',', b'speak', b'.', b'DOUBLEN',\n",
      "  b'First', b'Citizen', b':', b'NEWLINE', b'You', b'are', b'all',\n",
      "  b'resolved', b'rather', b'to', b'die', b'than', b'to', b'famish', b'?']]>\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tokenizer.detokenize(tokens)\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca8c1962-d3ae-4d57-8269-15c0ae11d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen :\n",
      "Before we proceed any further , hear me speak .\n",
      "\n",
      "All :\n",
      "Speak , speak .\n",
      "\n",
      "First Citizen :\n",
      "You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "def encode_newlines(txt: tf.Tensor):\n",
    "    \"\"\"Replace newline symbols with special tokens.\"\"\"\n",
    "    result = tf.strings.regex_replace(txt, \"\\n\\n\", config.DOUBLEN_TOKEN)\n",
    "    result = tf.strings.regex_replace(result, \"\\n\", config.NEWLINE_TOKEN)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cleanup_text(txt_tokens: tf.RaggedTensor):\n",
    "    \"\"\"Remove special tokens and concatenate the words into a coherent string.\"\"\"\n",
    "    result = tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1)\n",
    "    result = tf.strings.regex_replace(result, config.DOUBLEN_TOKEN, \"\\n\\n\")\n",
    "    result = tf.strings.regex_replace(result, config.NEWLINE_TOKEN, \"\\n\")\n",
    "    result = tf.strings.regex_replace(result, config.NEWLINE_TOKEN.strip(), \"\")\n",
    "    return result\n",
    "\n",
    "\n",
    "txt_clean = cleanup_text(txt_tokens)\n",
    "print(txt_clean[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the GPT, including functionality for cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(config.VOCAB_PATH)\n",
    "\n",
    "        vocab = pathlib.Path(config.VOCAB_PATH).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        strings = encode_newlines(strings)\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(config)\n",
    "tf.saved_model.save(tokenizer, config.TOKENIZER_PATH)\n",
    "reloaded_tokenizer = tf.saved_model.load(config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 167, 5422,   65, 4051,   78,  675, 5715, 2470,    0,   66,    8,\n",
       "          38]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizer.tokenize([\"Hello\\nTensorFlow!\\n\\n:]\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "TensorFlow !\n",
      "\n",
      ": ]\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizer.detokenize(tokens)\n",
    "print(round_trip[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5690d-59e3-45e5-ae17-5e3f6ff51f5b",
   "metadata": {},
   "source": [
    "## Save tokenized dataset\n",
    "Split tokenized dataset into batches and then separate into train and validation sets. Store the resulting tokenized datasets on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2e5758a-64a8-45ce-9861-2ae83282e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1,348,491\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(shakespeare_plays).numpy()\n",
    "n_tokens = tokens.shape[1]\n",
    "print(f\"Number of tokens: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fda270ce-c7d3-46cf-b8c9-a64ab372446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = config.MAX_TOKENS + 1\n",
    "indices = tf.random.uniform(\n",
    "    (config.N_SAMPLES,),\n",
    "    minval=0,\n",
    "    maxval=n_tokens - sample_len,\n",
    "    dtype=tf.dtypes.int32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2c466ea-4372-4436-94fe-f234c51fd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_from_index(i):\n",
    "    range = tf.range(i, i + sample_len, 1)\n",
    "    return tf.gather(tokens[0], range)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(indices).map(\n",
    "    get_sample_from_index, tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14627610-24ee-4f76-b890-c77ad1d5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(tokens[0])\n",
    "    .batch(config.MAX_TOKENS + 1, drop_remainder=True)\n",
    "    .shuffle(config.BUFFER_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d05b836-664a-41d6-bd01-677c7564bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will come into the chamber .\n",
      "\n",
      "FORD :\n",
      "Old woman ! what old woman ' s that ?\n",
      "\n",
      "MISTRESS FORD :\n",
      "Nay , it is my maid ' s aunt of Brentford .\n",
      "\n",
      "FORD :\n",
      "A witch , a quean , an old cozening quean ! Have I not\n",
      "forbid her my house ? She comes of errands , does\n",
      "she ? We are simple men ; we do not know what ' s\n",
      "brought to pass under the profession of\n",
      "fortune - telling . She works by charms , by spells ,\n",
      "by the figure , and such daubery as this is , beyond \n",
      "================================================================================\n",
      "this fault ?\n",
      "If on the first , how heinous e ' er it be ,\n",
      "To win thy after - love I pardon thee .\n",
      "\n",
      "DUKE OF AUMERLE :\n",
      "Then give me leave that I may turn the key ,\n",
      "That no man enter till my tale be done .\n",
      "\n",
      "HENRY BOLINGBROKE :\n",
      "Have thy desire .\n",
      "\n",
      "DUKE OF YORK :\n",
      "\n",
      "HENRY BOLINGBROKE :\n",
      "Villain , I ' ll make thee safe .\n",
      "\n",
      "DUKE OF AUMERLE :\n",
      "Stay thy revengeful hand ; thou hast no cause to fear .\n",
      "\n",
      "DUKE OF YORK :\n",
      "\n",
      "HENRY BOLINGBROKE :\n",
      "What is the matter , uncle ? speak ;\n",
      "Recover breath ;\n",
      "================================================================================\n",
      ", like one\n",
      "Who having into truth , by telling of it ,\n",
      "Made such a sinner of his memory ,\n",
      "To credit his own lie , he did believe\n",
      "He was indeed the duke ; out o ' the substitution\n",
      "And executing the outward face of royalty ,\n",
      "With all prerogative : hence his ambition growing - -\n",
      "Dost thou hear ?\n",
      "\n",
      "MIRANDA :\n",
      "Your tale , sir , would cure deafness .\n",
      "\n",
      "PROSPERO :\n",
      "To have no screen between this part he play ' d\n",
      "And him he play ' d it for , he needs will\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "examples = list(dataset.take(3))\n",
    "detokenized_examples = tokenizer.detokenize(examples).numpy()\n",
    "for ex in detokenized_examples:\n",
    "    print(ex.decode(\"utf-8\"))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bbfe93f-52fa-4f78-af48-0e6fff02b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = dataset.cardinality().numpy()\n",
    "val_size = int(n_tokens * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbfa03f4-8d3d-4923-9bfd-2db374bd5169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
       "array([b\"go off a little .\\nGo with him , sirrah .\\n\\nTOUCHSTONE :\\nCome , shepherd , let us make an honourable retreat ;\\nthough not with bag and baggage , yet with scrip and scrippage .\\n\\nCELIA :\\nDidst thou hear these verses ?\\n\\nROSALIND :\\nO , yes , I heard them all , and more too ; for some of\\nthem had in them more feet than the verses would bear .\\n\\nCELIA :\\nThat ' s no matter : the feet might bear the verses .\\n\\nROSALIND :\\nAy , but the feet were lame and could not bear NEWLINE\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize([examples[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc43ff1-e1f4-4118-b6dc-e9bfcb82cc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
