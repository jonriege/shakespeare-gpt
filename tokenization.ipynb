{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.python.ops import bert_tokenizer\n",
    "from tensorflow_text.tools.wordpiece_vocab import wordpiece_tokenizer_learner_lib as learner\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the source text from the concatenated [concatenated works of Shakespeare](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae3725-98c0-4d8c-a65f-11b45f5f4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.RAW_DATA_PATH, 'r') as file:\n",
    "    shakespeare_plays = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d48751-ee20-4603-b3aa-8b6c9bb1aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "sample = shakespeare_plays[:147]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26cad1-4cde-481e-9f37-cc3c9c1ba07f",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bb6677-5149-4191-81e7-75251b1cbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert_tokenizer.BasicTokenizer(**config.BERT_TOKENIZER_PARAMS)\n",
    "words_dataset = tokenizer.tokenize(shakespeare_plays)\n",
    "word_counts = learner.count_words(words_dataset)\n",
    "vocab = learner.learn(word_counts, config.VOCAB_SIZE, config.RESERVED_TOKENS, **config.LEARN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00f8691-e70b-4436-94c6-16abed4ca6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '$', '&', \"'\", ',', '-']\n",
      "['well', 'was', 'which', 'there', 'how', 'am', 'then', '##ed', '##ing', 'man']\n",
      "['##.', '##3', '##:', '##;', '##?', '##[', '##]', '##j', '##q', '##v']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(vocab[100:110])\n",
    "print(vocab[-10:])\n",
    "\n",
    "with open(config.VOCAB_PATH, \"w\") as f:\n",
    "    for token in vocab:\n",
    "        print(token, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571dd1f-41bd-4dfc-a890-a7a3b94a25ae",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bab5441-d04e-4751-bc92-c7cfc091a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.BertTokenizer(config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947351b8-d3ca-4c59-827e-715508bb0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[140, 606, 12, 196, 76, 1417, 178, 539, 8, 170, 53, 147, 10, 72, 12, 147,\n",
      "  8, 147, 10, 140, 606, 12, 47, 80, 72, 1917, 361, 45, 269, 115, 45, 4344,\n",
      "  14]]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample).merge_dims(-2, -1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18d6633-f624-4e86-ad8d-37e1400da6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen : you are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tf.gather(vocab, tokens)\n",
    "txt_tokens = tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1).numpy()[0].decode(\"utf-8\")\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the GPT, including functionality for cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=\" \", axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(config.VOCAB_PATH)\n",
    "\n",
    "        vocab = pathlib.Path(config.VOCAB_PATH).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(config)\n",
    "tf.saved_model.save(tokenizer, config.TOKENIZER_PATH)\n",
    "reloaded_tokenizer = tf.saved_model.load(config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 647,  650,  736,   63,  866, 2003, 4975,    4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizer.tokenize([\"Hello TensorFlow!\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizer.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5690d-59e3-45e5-ae17-5e3f6ff51f5b",
   "metadata": {},
   "source": [
    "## Save tokenized dataset\n",
    "Split tokenized dataset into batches and then separate into train and validation sets. Store the resulting tokenized datasets on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e5758a-64a8-45ce-9861-2ae83282e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1,167,156\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(shakespeare_plays).numpy()\n",
    "n_tokens = tokens.shape[1]\n",
    "print(f\"Number of tokens: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda270ce-c7d3-46cf-b8c9-a64ab372446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100_000\n",
    "sample_len = config.MAX_TOKENS + 1\n",
    "indices = tf.random.uniform((n_samples,), minval=0, maxval=n_tokens - sample_len, dtype=tf.dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cece7f54-89f1-48e2-8138-504317892b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140, 606,  12, ...,  62, 181,  10]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c7d72b1-5dba-4c75-b36d-c4ab809a3204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(129,), dtype=int64, numpy=\n",
       "array([ 140,  606,   12,  196,   76, 1417,  178,  539,    8,  170,   53,\n",
       "        147,   10,   72,   12,  147,    8,  147,   10,  140,  606,   12,\n",
       "         47,   80,   72, 1917,  361,   45,  269,  115,   45, 4344,   14,\n",
       "         72,   12, 1917,   10, 1917,   10,  140,  606,   12,  140,    8,\n",
       "         47,  119,  844,  861,   51, 1099,  651,   45,   43,  520,   10,\n",
       "         72,   12,   76,  119,    7,   36,    8,   76,  119,    7,   36,\n",
       "         10,  140,  606,   12,   96,  123,  400,   67,    8,   44,   76,\n",
       "          7,   91,   64, 2334,   93,   82,  188, 2712,   10,   51,    7,\n",
       "         36,   17, 4671, 3638, 2997,   14,   72,   12,   73,   99, 3376,\n",
       "         84,    7,   36,   13,   96,   54,   57,  201,   12,  177,    8,\n",
       "        177,    4,  216,  606,   12,  118,  252,    8,   87, 1464,   10,\n",
       "        140,  606,   12,   76,   80, 2035,  107,  215])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range = tf.range(0, 0 + sample_len, 1)\n",
    "tf.gather(tokens[0], range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2c466ea-4372-4436-94fe-f234c51fd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_from_index(i):\n",
    "    range = tf.range(i, i + sample_len, 1)\n",
    "    return tf.gather(tokens[0], range)\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(indices)\n",
    "    .map(get_sample_from_index, tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "14627610-24ee-4f76-b890-c77ad1d5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(tokens[0])\n",
    "    .batch(config.MAX_TOKENS + 1, drop_remainder=True)\n",
    "    .shuffle(config.BUFFER_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d05b836-664a-41d6-bd01-677c7564bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eunuch ; peace ! she hath betray ' d me and shall die the death . mardian : death of one person can be paid but once , and that she has discharged : what thou wouldst do is done unto thy hand : the last she spake was ' antony ! most noble antony ! ' then in the midst a tearing groan did break the name of antony ; it was divided between her heart and lips : she render ' d life , thy name so buried in her . mark antony : dead , then ? mardian : dead . mark antony : unarm , eros ; the long day ' s task is done , and we must sleep . that thou depart\n",
      "\n",
      "subject as i am , against thy oath and true allegiance sworn , should raise so great a power without his leave , or dare to bring thy force so near the court . york : buckingham : that is too much presumption on thy part : but if thy arms be to no other end , the king hath yielded unto thy demand : the duke of somerset is in the tower . york : upon thine honour , is he prisoner ? buckingham : upon mine honour , he is prisoner . york : then , buckingham , i do dismiss my powers . soldiers , i thank you all ; disperse yourselves ; meet me to - morrow in st .\n",
      "\n",
      ": demetrius : why , boy , although our mother , unadvised , gave you a dancing - rapier by your side , are you so desperate grown , to threat your friends ? go to ; have your lath glued within your sheath till you know better how to handle it . chiron : meanwhile , sir , with the little skill i have , full well shalt thou perceive how much i dare . demetrius : ay , boy , grow ye so brave ? aaron : demetrius : not i , till i have sheathed my rapier in his bosom and withal thrust these reproachful speeches down his throat that he hath\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = list(dataset.take(3))\n",
    "detokenized_examples = tokenizer.detokenize(examples).numpy()\n",
    "for ex in detokenized_examples:\n",
    "    print(ex.decode(\"utf-8\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bbfe93f-52fa-4f78-af48-0e6fff02b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = dataset.cardinality().numpy()\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba6dbe-0f97-45fc-8f3a-2b9d882f2168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
