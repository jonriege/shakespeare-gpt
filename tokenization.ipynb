{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.python.ops import bert_tokenizer\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    wordpiece_tokenizer_learner_lib as learner,\n",
    ")\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the source text from the concatenated [concatenated works of Shakespeare](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt). Replace newline symbols (\\n) with a special token so that information about the structure of the text is correctly tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae3725-98c0-4d8c-a65f-11b45f5f4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.RAW_DATA_PATH, \"r\") as file:\n",
    "    shakespeare_plays = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d48751-ee20-4603-b3aa-8b6c9bb1aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "sample = shakespeare_plays[:147]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9187a68e-5d92-402f-88be-6e18bd1961db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: NEWLINE Before we proceed any further, hear me speak. DOUBLEN All: NEWLINE Speak, speak. DOUBLEN First Citizen: NEWLINE You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "shakespeare_plays = shakespeare_plays.replace(\"\\n\\n\", config.DOUBLEN_TOKEN)\n",
    "shakespeare_plays = shakespeare_plays.replace(\"\\n\", config.NEWLINE_TOKEN)\n",
    "sample = shakespeare_plays[:185]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26cad1-4cde-481e-9f37-cc3c9c1ba07f",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bb6677-5149-4191-81e7-75251b1cbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert_tokenizer.BasicTokenizer(**config.BERT_TOKENIZER_PARAMS)\n",
    "words_dataset = tokenizer.tokenize(shakespeare_plays)\n",
    "word_counts = learner.count_words(words_dataset)\n",
    "vocab = learner.learn(\n",
    "    word_counts,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.RESERVED_TOKENS,\n",
    "    **config.LEARN_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00f8691-e70b-4436-94c6-16abed4ca6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';']\n",
      "['shall', 'are', 'To', 'thee', 'by', 'we', 'That', 'on', 'no', 'our']\n",
      "['##U', '##V', '##W', '##X', '##Z', '##[', '##]', '##j', '##q', '##v']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(vocab[100:110])\n",
    "print(vocab[-10:])\n",
    "\n",
    "with open(config.VOCAB_PATH, \"w\") as f:\n",
    "    for token in vocab:\n",
    "        print(token, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571dd1f-41bd-4dfc-a890-a7a3b94a25ae",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bab5441-d04e-4751-bc92-c7cfc091a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.BertTokenizer(\n",
    "    config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947351b8-d3ca-4c59-827e-715508bb0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[205, 698, 8, 65, 992, 105, 2366, 221, 641, 4, 222, 77, 184, 6, 66, 355,\n",
      "  8, 65, 804, 4, 184, 6, 66, 205, 698, 8, 65, 151, 101, 99, 2311, 467, 69,\n",
      "  325, 153, 69, 5270, 10]]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample).merge_dims(-2, -1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18d6633-f624-4e86-ad8d-37e1400da6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen : NEWLINE Before we proceed any further , hear me speak . DOUBLEN All : NEWLINE Speak , speak . DOUBLEN First Citizen : NEWLINE You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tf.gather(vocab, tokens)\n",
    "txt_tokens = (\n",
    "    tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1)\n",
    "    .numpy()[0]\n",
    "    .decode(\"utf-8\")\n",
    ")\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a17cd2d-3f7b-4bb8-8fc1-b685e76e9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'First', b'Citizen', b':', b'NEWLINE', b'Before', b'we', b'proceed',\n",
      "  b'any', b'further', b',', b'hear', b'me', b'speak', b'.', b'DOUBLEN',\n",
      "  b'All', b':', b'NEWLINE', b'Speak', b',', b'speak', b'.', b'DOUBLEN',\n",
      "  b'First', b'Citizen', b':', b'NEWLINE', b'You', b'are', b'all',\n",
      "  b'resolved', b'rather', b'to', b'die', b'than', b'to', b'famish', b'?']]>\n"
     ]
    }
   ],
   "source": [
    "txt_tokens = tokenizer.detokenize(tokens)\n",
    "print(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8c1962-d3ae-4d57-8269-15c0ae11d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen :\n",
      "Before we proceed any further , hear me speak .\n",
      "\n",
      "All :\n",
      "Speak , speak .\n",
      "\n",
      "First Citizen :\n",
      "You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "def encode_newlines(txt: tf.Tensor):\n",
    "    \"\"\"Replace newline symbols with special tokens.\"\"\"\n",
    "    result = tf.strings.regex_replace(txt, \"\\n\\n\", config.DOUBLEN_TOKEN)\n",
    "    result = tf.strings.regex_replace(result, \"\\n\", config.NEWLINE_TOKEN)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cleanup_text(txt_tokens: tf.RaggedTensor):\n",
    "    \"\"\"Remove special tokens and concatenate the words into a coherent string.\"\"\"\n",
    "    result = tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1)\n",
    "    result = tf.strings.regex_replace(result, config.DOUBLEN_TOKEN, \"\\n\\n\")\n",
    "    result = tf.strings.regex_replace(result, config.NEWLINE_TOKEN, \"\\n\")\n",
    "    result = tf.strings.regex_replace(result, config.NEWLINE_TOKEN.strip(), \"\")\n",
    "    return result\n",
    "\n",
    "\n",
    "txt_clean = cleanup_text(txt_tokens)\n",
    "print(txt_clean[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the GPT, including functionality for cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            config.VOCAB_PATH, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(config.VOCAB_PATH)\n",
    "\n",
    "        vocab = pathlib.Path(config.VOCAB_PATH).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        strings = encode_newlines(strings)\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(config)\n",
    "tf.saved_model.save(tokenizer, config.TOKENIZER_PATH)\n",
    "reloaded_tokenizer = tf.saved_model.load(config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 167, 5422,   65, 4051,   78,  675, 5715, 2470,    0,   66,    8,\n",
       "          38]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizer.tokenize([\"Hello\\nTensorFlow!\\n\\n:]\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "TensorFlow !\n",
      "\n",
      ": ]\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizer.detokenize(tokens)\n",
    "print(round_trip[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5690d-59e3-45e5-ae17-5e3f6ff51f5b",
   "metadata": {},
   "source": [
    "## Save tokenized dataset\n",
    "Split tokenized dataset into batches and then separate into train and validation sets. Store the resulting tokenized datasets on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e5758a-64a8-45ce-9861-2ae83282e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1,348,491\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(shakespeare_plays).numpy()\n",
    "n_tokens = tokens.shape[1]\n",
    "print(f\"Number of tokens: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fda270ce-c7d3-46cf-b8c9-a64ab372446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = config.MAX_TOKENS + 1\n",
    "indices = tf.random.uniform(\n",
    "    (config.N_SAMPLES,),\n",
    "    minval=0,\n",
    "    maxval=n_tokens - sample_len,\n",
    "    dtype=tf.dtypes.int32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2c466ea-4372-4436-94fe-f234c51fd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_from_index(i):\n",
    "    range = tf.range(i, i + sample_len, 1)\n",
    "    return tf.gather(tokens[0], range)\n",
    "\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(indices)\n",
    "    .map(get_sample_from_index, tf.data.AUTOTUNE)\n",
    "    .shuffle(config.BUFFER_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d05b836-664a-41d6-bd01-677c7564bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will live , resolve it you .\n",
      "Sharp physic is the last : but , O you powers\n",
      "That give heaven countless eyes to view men ' s acts ,\n",
      "Why cloud they not their sights perpetually ,\n",
      "If this be true , which makes me pale to read it ?\n",
      "Fair glass of light , I loved you , and could still ,\n",
      "Were not this glorious casket stored with ill :\n",
      "But I must tell you , now my thoughts revolt\n",
      "For he ' s no man on whom perfections wait\n",
      "That , knowing sin within , will touch the gate .\n",
      "You are a fair\n",
      "================================================================================\n",
      "##er than mankind .\n",
      "The gods confound - - hear me , you good gods all - -\n",
      "The Athenians both within and out that wall !\n",
      "And grant , as Timon grows , his hate may grow\n",
      "To the whole race of mankind , high and low ! Amen .\n",
      "\n",
      "First Servant :\n",
      "Hear you , master steward , where ' s our master ?\n",
      "Are we undone ? cast off ? nothing remaining ?\n",
      "\n",
      "FLAVIUS :\n",
      "Alack , my fellows , what should I say to you ?\n",
      "Let me be recorded by the righteous gods ,\n",
      "I am as poor as you .\n",
      "\n",
      "First Servant :\n",
      "Such a\n",
      "================================================================================\n",
      "me cast my love on him ?\n",
      "\n",
      "LUCETTA :\n",
      "Ay , if you thought your love not cast away .\n",
      "\n",
      "JULIA :\n",
      "Why he , of all the rest , hath never moved me .\n",
      "\n",
      "LUCETTA :\n",
      "Yet he , of all the rest , I think , best loves ye .\n",
      "\n",
      "JULIA :\n",
      "His little speaking shows his love but small .\n",
      "\n",
      "LUCETTA :\n",
      "Fire that ' s closest kept burns most of all .\n",
      "\n",
      "JULIA :\n",
      "They do not love that do not show their love .\n",
      "\n",
      "LUCETTA :\n",
      "O , they love least that let men know their love .\n",
      "\n",
      "JULIA :\n",
      "I would I knew his mind\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "examples = list(dataset.take(3))\n",
    "detokenized_examples = tokenizer.detokenize(examples).numpy()\n",
    "for ex in detokenized_examples:\n",
    "    print(ex.decode(\"utf-8\"))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bbfe93f-52fa-4f78-af48-0e6fff02b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = dataset.cardinality().numpy()\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca6a47-ec25-4630-ae9e-680f6133f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
